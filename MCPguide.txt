 Model Context Protocol
An open protocol that enables seamless integration between LLM applications and external data sources and tools.

    Verified

3k followers

    https://modelcontextprotocol.io

README.md
Model Context Protocol

MCP Logo

A protocol for seamless integration between LLM applications and external data sources

Documentation | Specification | Discussions

The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.
Getting Started

    📚 Read the Documentation for guides and tutorials
    🔍 Review the Specification for protocol details
    💻 Use our SDKs to start building:
        TypeScript SDK
        Python SDK

Project Structure

    specification - Protocol specification and documentation
    typescript-sdk - TypeScript implementation
    python-sdk - Python implementation
    docs - User documentation and guides
    create-python-server - Python server template
    create-typescript-server - TypeScript server template
    servers - List of maintained servers

Contributing

We welcome contributions of all kinds! Whether you want to fix bugs, improve documentation, or propose new features, please see our contributing guide to get started.

Have questions? Join the discussion in our community forum.
About
The Model Context Protocol is an open source project run by Anthropic, PBC. and open to contributions from the entire community.

Get Started
Introduction

Get started with the Model Context Protocol (MCP)

MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.
​
Why MCP?

MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides:

    A growing list of pre-built integrations that your LLM can directly plug into
    The flexibility to switch between LLM providers and vendors
    Best practices for securing your data within your infrastructure

​
General architecture

At its core, MCP follows a client-server architecture where a host application can connect to multiple servers:
Internet
Your Computer
MCP Protocol
MCP Protocol
MCP Protocol
Web APIs
Remote
Service C
MCP Host
(Claude, IDEs, Tools)
MCP Server A
MCP Server B
MCP Server C
Local
Data Source A
Local
Data Source B

    MCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP
    MCP Clients: Protocol clients that maintain 1:1 connections with servers
    MCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol
    Local Data Sources: Your computer’s files, databases, and services that MCP servers can securely access
    Remote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to

Quickstart

Get started with building your first MCP server and connecting it to a host

In this tutorial, we’ll build a simple MCP weather server and connect it to a host, Claude for Desktop. We’ll start with a basic setup, and then progress to more complex use cases.
​
What we’ll be building

Many LLMs (including Claude) do not currently have the ability to fetch the forecast and severe weather alerts. Let’s use MCP to solve that!

We’ll build a server that exposes two tools: get-alerts and get-forecast. Then we’ll connect the server to an MCP host (in this case, Claude for Desktop):

Servers can connect to any client. We’ve chosen Claude for Desktop here for simplicity, but we also have guides on building your own client as well as a list of other clients here.
​
Core MCP Concepts

MCP servers can provide three main types of capabilities:

    Resources: File-like data that can be read by clients (like API responses or file contents)
    Tools: Functions that can be called by the LLM (with user approval)
    Prompts: Pre-written templates that help users accomplish specific tasks

This tutorial will primarily focus on tools.

Let’s get started with building our weather server! You can find the complete code for what we’ll be building here.
Prerequisite knowledge

This quickstart assumes you have familiarity with:

    Python
    LLMs like Claude

System requirements

For Python, make sure you have Python 3.9 or higher installed.
Set up your environment

First, let’s install uv and set up our Python project and environment:
mac/osx:
curl -LsSf https://astral.sh/uv/install.sh | sh

windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

Make sure to restart your terminal afterwards to ensure that the uv command gets picked up.

Now, let’s create and set up our project:

mac/osx:
# Create a new directory for our project
uv init weather
cd weather

# Create virtual environment and activate it
uv venv
source .venv/bin/activate

# Install dependencies
uv add mcp httpx 

# Remove template file
rm hello.py

# Create our files
mkdir -p src/weather
touch src/weather/__init__.py
touch src/weather/server.py

windows:
# Create a new directory for our project
uv init weather
cd weather

# Create virtual environment and activate it
uv venv
.venv\Scripts\activate

# Install dependencies
uv add mcp httpx

# Clean up boilerplate code
rm hello.py

# Create our files
md src
md src\weather
new-item src\weather\__init__.py
new-item src\weather\server.py

Add this code to pyproject.toml:

...rest of config

[build-system]
requires = [ "hatchling",]
build-backend = "hatchling.build"

[project.scripts]
weather = "weather:main"

Add this code to __init__.py:
src/weather/__init__.py

from . import server
import asyncio

def main():
    """Main entry point for the package."""
    asyncio.run(server.main())

# Optionally expose other important items at package level
__all__ = ['main', 'server']

Now let’s dive into building your server.
Building your server
Importing packages

Add these to the top of your server.py:

from typing import Any
import asyncio
import httpx
from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

Setting up the instance

Then initialize the server instance and the base URL for the NWS API:

NWS_API_BASE = "https://api.weather.gov"
USER_AGENT = "weather-app/1.0"

server = Server("weather")

Implementing tool listing

We need to tell clients what tools are available. The list_tools() decorator registers this handler:

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """
    List available tools.
    Each tool specifies its arguments using JSON Schema validation.
    """
    return [
        types.Tool(
            name="get-alerts",
            description="Get weather alerts for a state",
            inputSchema={
                "type": "object",
                "properties": {
                    "state": {
                        "type": "string",
                        "description": "Two-letter state code (e.g. CA, NY)",
                    },
                },
                "required": ["state"],
            },
        ),
        types.Tool(
            name="get-forecast",
            description="Get weather forecast for a location",
            inputSchema={
                "type": "object",
                "properties": {
                    "latitude": {
                        "type": "number",
                        "description": "Latitude of the location",
                    },
                    "longitude": {
                        "type": "number",
                        "description": "Longitude of the location",
                    },
                },
                "required": ["latitude", "longitude"],
            },
        ),
    ]

This defines our two tools: get-alerts and get-forecast.
Helper functions

Next, let’s add our helper functions for querying and formatting the data from the National Weather Service API:

async def make_nws_request(client: httpx.AsyncClient, url: str) -> dict[str, Any] | None:
    """Make a request to the NWS API with proper error handling."""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }

    try:
        response = await client.get(url, headers=headers, timeout=30.0)
        response.raise_for_status()
        return response.json()
    except Exception:
        return None

def format_alert(feature: dict) -> str:
    """Format an alert feature into a concise string."""
    props = feature["properties"]
    return (
        f"Event: {props.get('event', 'Unknown')}\n"
        f"Area: {props.get('areaDesc', 'Unknown')}\n"
        f"Severity: {props.get('severity', 'Unknown')}\n"
        f"Status: {props.get('status', 'Unknown')}\n"
        f"Headline: {props.get('headline', 'No headline')}\n"
        "---"
    )

Implementing tool execution

The tool execution handler is responsible for actually executing the logic of each tool. Let’s add it:

@server.call_tool()
async def handle_call_tool(
    name: str, arguments: dict | None
) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:
    """
    Handle tool execution requests.
    Tools can fetch weather data and notify clients of changes.
    """
    if not arguments:
        raise ValueError("Missing arguments")
  
    if name == "get-alerts":
        state = arguments.get("state")
        if not state:
            raise ValueError("Missing state parameter")

        # Convert state to uppercase to ensure consistent format
        state = state.upper()
        if len(state) != 2:
            raise ValueError("State must be a two-letter code (e.g. CA, NY)")

        async with httpx.AsyncClient() as client:
            alerts_url = f"{NWS_API_BASE}/alerts?area={state}"
            alerts_data = await make_nws_request(client, alerts_url)

            if not alerts_data:
                return [types.TextContent(type="text", text="Failed to retrieve alerts data")]

            features = alerts_data.get("features", [])
            if not features:
                return [types.TextContent(type="text", text=f"No active alerts for {state}")]

            # Format each alert into a concise string
            formatted_alerts = [format_alert(feature) for feature in features[:20]] # only take the first 20 alerts
            alerts_text = f"Active alerts for {state}:\n\n" + "\n".join(formatted_alerts)

            return [
                types.TextContent(
                    type="text",
                    text=alerts_text
                )
            ]
    elif name == "get-forecast":
        try:
            latitude = float(arguments.get("latitude"))
            longitude = float(arguments.get("longitude"))
        except (TypeError, ValueError):
            return [types.TextContent(
                type="text",
                text="Invalid coordinates. Please provide valid numbers for latitude and longitude."
            )]
            
        # Basic coordinate validation
        if not (-90 <= latitude <= 90) or not (-180 <= longitude <= 180):
            return [types.TextContent(
                type="text",
                text="Invalid coordinates. Latitude must be between -90 and 90, longitude between -180 and 180."
            )]

        async with httpx.AsyncClient() as client:
            # First get the grid point
            lat_str = f"{latitude}"
            lon_str = f"{longitude}"
            points_url = f"{NWS_API_BASE}/points/{lat_str},{lon_str}"
            points_data = await make_nws_request(client, points_url)

            if not points_data:
                return [types.TextContent(type="text", text=f"Failed to retrieve grid point data for coordinates: {latitude}, {longitude}. This location may not be supported by the NWS API (only US locations are supported).")]

            # Extract forecast URL from the response
            properties = points_data.get("properties", {})
            forecast_url = properties.get("forecast")
            
            if not forecast_url:
                return [types.TextContent(type="text", text="Failed to get forecast URL from grid point data")]

            # Get the forecast
            forecast_data = await make_nws_request(client, forecast_url)
            
            if not forecast_data:
                return [types.TextContent(type="text", text="Failed to retrieve forecast data")]

            # Format the forecast periods
            periods = forecast_data.get("properties", {}).get("periods", [])
            if not periods:
                return [types.TextContent(type="text", text="No forecast periods available")]

            # Format each period into a concise string
            formatted_forecast = []
            for period in periods:
                forecast_text = (
                    f"{period.get('name', 'Unknown')}:\n"
                    f"Temperature: {period.get('temperature', 'Unknown')}°{period.get('temperatureUnit', 'F')}\n"
                    f"Wind: {period.get('windSpeed', 'Unknown')} {period.get('windDirection', '')}\n"
                    f"{period.get('shortForecast', 'No forecast available')}\n"
                    "---"
                )
                formatted_forecast.append(forecast_text)

            forecast_text = f"Forecast for {latitude}, {longitude}:\n\n" + "\n".join(formatted_forecast)

            return [types.TextContent(
                type="text",
                text=forecast_text
            )]
    else:
        raise ValueError(f"Unknown tool: {name}")

Running the server

Finally, implement the main function to run the server:

async def main():
    # Run the server using stdin/stdout streams
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="weather",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )

# This is needed if you'd like to connect to a custom client
if __name__ == "__main__":
    asyncio.run(main())

Your server is complete! Run uv run src/weather/server.py to confirm that everything’s working.

Let’s now test your server from an existing MCP host, Claude for Desktop.
Testing your server with Claude for Desktop

Claude for Desktop is not yet available on Linux. Linux users can proceed to the Building a client tutorial to build an MCP client that connects to the server we just built.

First, make sure you have Claude for Desktop installed. You can install the latest version here. If you already have Claude for Desktop, make sure it’s updated to the latest version.

We’ll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at ~/Library/Application Support/Claude/claude_desktop_config.json in a text editor. Make sure to create the file if it doesn’t exist.

For example, if you have VS Code installed:
mac/osx:
code ~/Library/Application\ Support/Claude/claude_desktop_config.json

windows:
code $env:AppData\Claude\claude_desktop_config.json

You’ll then add your servers in the mcpServers key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.

In this case, we’ll add our single weather server like so:


mac/osx (python):

{
    "mcpServers": {
        "weather": {
            "command": "uv",
            "args": [
                "--directory",
                "/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather",
                "run",
                "weather"
            ]
        }
    }
}

windows (python):

{
    "mcpServers": {
        "weather": {
            "command": "uv",
            "args": [
                "--directory",
                "C:\\ABSOLUTE\PATH\TO\PARENT\FOLDER\weather",
                "run",
                "weather"
            ]
        }
    }
}


Make sure you pass in the absolute path to your server.

This tells Claude for Desktop:

    There’s an MCP server named “weather”
    To launch it by running uv --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather run weather

Save the file, and restart Claude for Desktop.
​
Test with commands

Let’s make sure Claude for Desktop is picking up the two tools we’ve exposed in our weather server. You can do this by looking for the hammer icon:

After clicking on the hammer icon, you should see two tools listed:

If your server isn’t being picked up by Claude for Desktop, proceed to the Troubleshooting section for debugging tips.

If the hammer icon has shown up, you can now test your server by running the following commands in Claude for Desktop:

    What’s the weather in Sacramento?
    What are the active weather alerts in Texas?

Since this is the US National Weather service, the queries will only work for US locations.
​
What’s happening under the hood

When you ask a question:

    The client sends your question to Claude
    Claude analyzes the available tools and decides which one(s) to use
    The client executes the chosen tool(s) through the MCP server
    The results are sent back to Claude
    Claude formulates a natural language response
    The response is displayed to you!

Get Started
Examples

A list of example servers and implementations

This page showcases various Model Context Protocol (MCP) servers that demonstrate the protocol’s capabilities and versatility. These servers enable Large Language Models (LLMs) to securely access tools and data sources.
​
Reference implementations

These official reference servers demonstrate core MCP features and SDK usage:
​
Data and file systems

    Filesystem - Secure file operations with configurable access controls
    PostgreSQL - Read-only database access with schema inspection capabilities
    SQLite - Database interaction and business intelligence features
    Google Drive - File access and search capabilities for Google Drive

​
Development tools

    Git - Tools to read, search, and manipulate Git repositories
    GitHub - Repository management, file operations, and GitHub API integration
    GitLab - GitLab API integration enabling project management
    Sentry - Retrieving and analyzing issues from Sentry.io

​
Web and browser automation

    Brave Search - Web and local search using Brave’s Search API
    Fetch - Web content fetching and conversion optimized for LLM usage
    Puppeteer - Browser automation and web scraping capabilities

​
Productivity and communication

    Slack - Channel management and messaging capabilities
    Google Maps - Location services, directions, and place details
    Memory - Knowledge graph-based persistent memory system

​
AI and specialized tools

    EverArt - AI image generation using various models
    Sequential Thinking - Dynamic problem-solving through thought sequences
    AWS KB Retrieval - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime

​
Official integrations

These MCP servers are maintained by companies for their platforms:

    Axiom - Query and analyze logs, traces, and event data using natural language
    Browserbase - Automate browser interactions in the cloud
    Cloudflare - Deploy and manage resources on the Cloudflare developer platform
    E2B - Execute code in secure cloud sandboxes
    Neon - Interact with the Neon serverless Postgres platform
    Obsidian Markdown Notes - Read and search through Markdown notes in Obsidian vaults
    Qdrant - Implement semantic memory using the Qdrant vector search engine
    Raygun - Access crash reporting and monitoring data
    Search1API - Unified API for search, crawling, and sitemaps
    Tinybird - Interface with the Tinybird serverless ClickHouse platform

​
Community highlights

A growing ecosystem of community-developed servers extends MCP’s capabilities:

    Docker - Manage containers, images, volumes, and networks
    Kubernetes - Manage pods, deployments, and services
    Linear - Project management and issue tracking
    Snowflake - Interact with Snowflake databases
    Spotify - Control Spotify playback and manage playlists
    Todoist - Task management integration

    Note: Community servers are untested and should be used at your own risk. They are not affiliated with or endorsed by Anthropic.

For a complete list of community servers, visit the MCP Servers Repository.
​
Getting started
​
Using reference servers

TypeScript-based servers can be used directly with npx:

npx -y @modelcontextprotocol/server-memory

Python-based servers can be used with uvx (recommended) or pip:

# Using uvx
uvx mcp-server-git

# Using pip
pip install mcp-server-git
python -m mcp_server_git

​
Configuring with Claude

To use an MCP server with Claude, add it to your configuration:

{
  "mcpServers": {
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/files"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "<YOUR_TOKEN>"
      }
    }
  }
}

​
Additional resources

    MCP Servers Repository - Complete collection of reference implementations and community servers
    Awesome MCP Servers - Curated list of MCP servers
    MCP CLI - Command-line inspector for testing MCP servers
    MCP Get - Tool for installing and managing MCP servers

Visit our GitHub Discussions to engage with the MCP community.

Was this page helpful?

Get Started
Clients

A list of applications that support MCP integrations

This page provides an overview of applications that support the Model Context Protocol (MCP). Each client may support different MCP features, allowing for varying levels of integration with MCP servers.
​
Feature support matrix
Client	Resources	Prompts	Tools	Sampling	Roots	Notes
Claude Desktop App	✅	✅	✅	❌	❌	Full support for all MCP features
Zed	❌	✅	❌	❌	❌	Prompts appear as slash commands
Sourcegraph Cody	✅	❌	❌	❌	❌	Supports resources through OpenCTX
Firebase Genkit	⚠️	✅	✅	❌	❌	Supports resource list and lookup through tools.
Continue	✅	✅	✅	❌	❌	Full support for all MCP features
GenAIScript	❌	❌	✅	❌	❌	Supports tools.
​
Client details
​
Claude Desktop App

The Claude desktop application provides comprehensive support for MCP, enabling deep integration with local tools and data sources.

Key features:

    Full support for resources, allowing attachment of local files and data
    Support for prompt templates
    Tool integration for executing commands and scripts
    Local server connections for enhanced privacy and security

    ⓘ Note: The Claude.ai web application does not currently support MCP. MCP features are only available in the desktop application.

​
Zed

Zed is a high-performance code editor with built-in MCP support, focusing on prompt templates and tool integration.

Key features:

    Prompt templates surface as slash commands in the editor
    Tool integration for enhanced coding workflows
    Tight integration with editor features and workspace context
    Does not support MCP resources

​
Sourcegraph Cody

Cody is Sourcegraph’s AI coding assistant, which implements MCP through OpenCTX.

Key features:

    Support for MCP resources
    Integration with Sourcegraph’s code intelligence
    Uses OpenCTX as an abstraction layer
    Future support planned for additional MCP features

​
Firebase Genkit

Genkit is Firebase’s SDK for building and integrating GenAI features into applications. The genkitx-mcp plugin enables consuming MCP servers as a client or creating MCP servers from Genkit tools and prompts.

Key features:

    Client support for tools and prompts (resources partially supported)
    Rich discovery with support in Genkit’s Dev UI playground
    Seamless interoperability with Genkit’s existing tools and prompts
    Works across a wide variety of GenAI models from top providers

​
Continue

Continue is an open-source AI code assistant, with built-in support for all MCP features.

Key features

    Type ”@” to mention MCP resources
    Prompt templates surface as slash commands
    Use both built-in and MCP tools directly in chat
    Supports VS Code and JetBrains IDEs, with any LLM

​
GenAIScript

Programmatically assemble prompts for LLMs using GenAIScript (in JavaScript). Orchestrate LLMs, tools, and data in JavaScript.

Key features:

    JavaScript toolbox to work with prompts
    Abstraction to make it easy and productive
    Seamless Visual Studio Code integration

​
Adding MCP support to your application

If you’ve added MCP support to your application, we encourage you to submit a pull request to add it to this list. MCP integration can provide your users with powerful contextual AI capabilities and make your application part of the growing MCP ecosystem.

Benefits of adding MCP support:

    Enable users to bring their own context and tools
    Join a growing ecosystem of interoperable AI applications
    Provide users with flexible integration options
    Support local-first AI workflows

To get started with implementing MCP in your application, check out our Python or TypeScript SDK Documentation
​
Tutorials
Building MCP clients

Learn how to build your first client in MCP

In this tutorial, you’ll learn how to build a LLM-powered chatbot client that connects to MCP servers. It helps to have gone through the Quickstart tutorial that guides you through the basic of building your first server.

You can find the complete code for this tutorial here.
System Requirements

Before starting, ensure your system meets these requirements:

    Mac or Windows computer
    Latest Python version installed
    Latest version of uv installed

Setting Up Your Environment

First, create a new Python project with uv:

# Create project directory
uv init mcp-client
cd mcp-client

# Create virtual environment
uv venv

# Activate virtual environment
# On Windows:
.venv\Scripts\activate
# On Unix or MacOS:
source .venv/bin/activate

# Install required packages
uv add mcp anthropic python-dotenv

# Remove boilerplate files
rm hello.py

# Create our main file
touch client.py

Setting Up Your API Key

You’ll need an Anthropic API key from the Anthropic Console.

Create a .env file to store it:

# Create .env file
touch .env

Add your key to the .env file:

ANTHROPIC_API_KEY=<your key here>

Add .env to your .gitignore:

echo ".env" >> .gitignore

Make sure you keep your ANTHROPIC_API_KEY secure!
Creating the Client
Basic Client Structure

First, let’s set up our imports and create the basic client class:

import asyncio
from typing import Optional
from contextlib import AsyncExitStack

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()  # load environment variables from .env

class MCPClient:
    def __init__(self):
        # Initialize session and client objects
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.anthropic = Anthropic()
    # methods will go here

Server Connection Management

Next, we’ll implement the method to connect to an MCP server:

async def connect_to_server(self, server_script_path: str):
    """Connect to an MCP server
    
    Args:
        server_script_path: Path to the server script (.py or .js)
    """
    is_python = server_script_path.endswith('.py')
    is_js = server_script_path.endswith('.js')
    if not (is_python or is_js):
        raise ValueError("Server script must be a .py or .js file")
        
    command = "python" if is_python else "node"
    server_params = StdioServerParameters(
        command=command,
        args=[server_script_path],
        env=None
    )
    
    stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
    self.stdio, self.write = stdio_transport
    self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
    
    await self.session.initialize()
    
    # List available tools
    response = await self.session.list_tools()
    tools = response.tools
    print("\nConnected to server with tools:", [tool.name for tool in tools])

Query Processing Logic

Now let’s add the core functionality for processing queries and handling tool calls:

async def process_query(self, query: str) -> str:
    """Process a query using Claude and available tools"""
    messages = [
        {
            "role": "user",
            "content": query
        }
    ]

    response = await self.session.list_tools()
    available_tools = [{ 
        "name": tool.name,
        "description": tool.description,
        "input_schema": tool.inputSchema
    } for tool in response.tools]

    # Initial Claude API call
    response = self.anthropic.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1000,
        messages=messages,
        tools=available_tools
    )

    # Process response and handle tool calls
    tool_results = []
    final_text = []

    for content in response.content:
        if content.type == 'text':
            final_text.append(content.text)
        elif content.type == 'tool_use':
            tool_name = content.name
            tool_args = content.input
            
            # Execute tool call
            result = await self.session.call_tool(tool_name, tool_args)
            tool_results.append({"call": tool_name, "result": result})
            final_text.append(f"[Calling tool {tool_name} with args {tool_args}]")

            # Continue conversation with tool results
            if hasattr(content, 'text') and content.text:
                messages.append({
                  "role": "assistant",
                  "content": content.text
                })
            messages.append({
                "role": "user", 
                "content": result.content
            })

            # Get next response from Claude
            response = self.anthropic.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=messages,
            )

            final_text.append(response.content[0].text)

    return "\n".join(final_text)

Interactive Chat Interface

Now we’ll add the chat loop and cleanup functionality:

async def chat_loop(self):
    """Run an interactive chat loop"""
    print("\nMCP Client Started!")
    print("Type your queries or 'quit' to exit.")
    
    while True:
        try:
            query = input("\nQuery: ").strip()
            
            if query.lower() == 'quit':
                break
                
            response = await self.process_query(query)
            print("\n" + response)
                
        except Exception as e:
            print(f"\nError: {str(e)}")

async def cleanup(self):
    """Clean up resources"""
    await self.exit_stack.aclose()

Main Entry Point

Finally, we’ll add the main execution logic:

async def main():
    if len(sys.argv) < 2:
        print("Usage: python client.py <path_to_server_script>")
        sys.exit(1)
        
    client = MCPClient()
    try:
        await client.connect_to_server(sys.argv[1])
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    import sys
    asyncio.run(main())

You can find the complete client.py file here.
Key Components Explained
1. Client Initialization

    The MCPClient class initializes with session management and API clients
    Uses AsyncExitStack for proper resource management
    Configures the Anthropic client for Claude interactions

2. Server Connection

    Supports both Python and Node.js servers
    Validates server script type
    Sets up proper communication channels
    Initializes the session and lists available tools

3. Query Processing

    Maintains conversation context
    Handles Claude’s responses and tool calls
    Manages the message flow between Claude and tools
    Combines results into a coherent response

4. Interactive Interface

    Provides a simple command-line interface
    Handles user input and displays responses
    Includes basic error handling
    Allows graceful exit

5. Resource Management

    Proper cleanup of resources
    Error handling for connection issues
    Graceful shutdown procedures

Common Customization Points

    Tool Handling
        Modify process_query() to handle specific tool types
        Add custom error handling for tool calls
        Implement tool-specific response formatting

    Response Processing
        Customize how tool results are formatted
        Add response filtering or transformation
        Implement custom logging

    User Interface
        Add a GUI or web interface
        Implement rich console output
        Add command history or auto-completion

Running the Client

To run your client with any MCP server:

uv run client.py path/to/server.py # python server
uv run client.py path/to/build/index.js # node server

If you’re continuing the weather tutorial from the quickstart, your command might look something like this: python client.py .../weather/src/weather/server.py

The client will:

    Connect to the specified server
    List available tools
    Start an interactive chat session where you can:
        Enter queries
        See tool executions
        Get responses from Claude

Here’s an example of what it should look like if connected to the weather server from the quickstart:
How It Works

When you submit a query:

    The client gets the list of available tools from the server
    Your query is sent to Claude along with tool descriptions
    Claude decides which tools (if any) to use
    The client executes any requested tool calls through the server
    Results are sent back to Claude
    Claude provides a natural language response
    The response is displayed to you

Best practices

    Error Handling
        Always wrap tool calls in try-catch blocks
        Provide meaningful error messages
        Gracefully handle connection issues

    Resource Management
        Use AsyncExitStack for proper cleanup
        Close connections when done
        Handle server disconnections

    Security
        Store API keys securely in .env
        Validate server responses
        Be cautious with tool permissions

Troubleshooting
Server Path Issues

    Double-check the path to your server script is correct
    Use the absolute path if the relative path isn’t working
    For Windows users, make sure to use forward slashes (/) or escaped backslashes (\) in the path
    Verify the server file has the correct extension (.py for Python or .js for Node.js)

Example of correct path usage:

# Relative path
uv run client.py ./server/weather.py

# Absolute path
uv run client.py /Users/username/projects/mcp-server/weather.py

# Windows path (either format works)
uv run client.py C:/projects/mcp-server/weather.py
uv run client.py C:\\projects\\mcp-server\\weather.py

Response Timing

    The first response might take up to 30 seconds to return
    This is normal and happens while:
        The server initializes
        Claude processes the query
        Tools are being executed
    Subsequent responses are typically faster
    Don’t interrupt the process during this initial waiting period

Common Error Messages

If you see:

    FileNotFoundError: Check your server path
    Connection refused: Ensure the server is running and the path is correct
    Tool execution failed: Verify the tool’s required environment variables are set
    Timeout error: Consider increasing the timeout in your client configuration

​
Next steps
Example servers

Check out our gallery of official MCP servers and implementations
Clients

View the list of clients that support MCP integrations
Building MCP with LLMs

Learn how to use LLMs like Claude to speed up your MCP development
Core architecture

Understand how MCP connects clients, servers, and LLMs

Tutorials
Building MCP with LLMs

Speed up your MCP development using LLMs such as Claude!

This guide will help you use LLMs to help you build custom Model Context Protocol (MCP) servers and clients. We’ll be focusing on Claude for this tutorial, but you can do this with any frontier LLM.
​
Preparing the documentation

Before starting, gather the necessary documentation to help Claude understand MCP:

    Visit https://modelcontextprotocol.io/llms-full.txt and copy the full documentation text
    Navigate to either the MCP TypeScript SDK or Python SDK repository
    Copy the README files and other relevant documentation
    Paste these documents into your conversation with Claude

​
Describing your server

Once you’ve provided the documentation, clearly describe to Claude what kind of server you want to build. Be specific about:

    What resources your server will expose
    What tools it will provide
    Any prompts it should offer
    What external systems it needs to interact with

For example:

Build an MCP server that:
- Connects to my company's PostgreSQL database
- Exposes table schemas as resources
- Provides tools for running read-only SQL queries
- Includes prompts for common data analysis tasks

​
Working with Claude

When working with Claude on MCP servers:

    Start with the core functionality first, then iterate to add more features
    Ask Claude to explain any parts of the code you don’t understand
    Request modifications or improvements as needed
    Have Claude help you test the server and handle edge cases

Claude can help implement all the key MCP features:

    Resource management and exposure
    Tool definitions and implementations
    Prompt templates and handlers
    Error handling and logging
    Connection and transport setup

​
Best practices

When building MCP servers with Claude:

    Break down complex servers into smaller pieces
    Test each component thoroughly before moving on
    Keep security in mind - validate inputs and limit access appropriately
    Document your code well for future maintenance
    Follow MCP protocol specifications carefully

​
Next steps

After Claude helps you build your server:

    Review the generated code carefully
    Test the server with the MCP Inspector tool
    Connect it to Claude.app or other MCP clients
    Iterate based on real usage and feedback

Remember that Claude can help you modify and improve your server as requirements change over time.

Need more guidance? Just ask Claude specific questions about implementing MCP features or troubleshooting issues that arise.


Tutorials
Debugging

A comprehensive guide to debugging Model Context Protocol (MCP) integrations

Effective debugging is essential when developing MCP servers or integrating them with applications. This guide covers the debugging tools and approaches available in the MCP ecosystem.

This guide is for macOS. Guides for other platforms are coming soon.
​
Debugging tools overview

MCP provides several tools for debugging at different levels:

    MCP Inspector
        Interactive debugging interface
        Direct server testing
        See the Inspector guide for details

    Claude Desktop Developer Tools
        Integration testing
        Log collection
        Chrome DevTools integration

    Server Logging
        Custom logging implementations
        Error tracking
        Performance monitoring

​
Debugging in Claude Desktop
​
Checking server status

The Claude.app interface provides basic server status information:

    Click the icon to view:
        Connected servers
        Available prompts and resources

    Click the icon to view:
        Tools made available to the model

​
Viewing logs

Review detailed MCP logs from Claude Desktop:

# Follow logs in real-time
tail -n 20 -f ~/Library/Logs/Claude/mcp*.log

The logs capture:

    Server connection events
    Configuration issues
    Runtime errors
    Message exchanges

​
Using Chrome DevTools

Access Chrome’s developer tools inside Claude Desktop to investigate client-side errors:

    Enable DevTools:

jq '.allowDevTools = true' ~/Library/Application\ Support/Claude/developer_settings.json > tmp.json \
  && mv tmp.json ~/Library/Application\ Support/Claude/developer_settings.json

    Open DevTools: Command-Option-Shift-i

Note: You’ll see two DevTools windows:

    Main content window
    App title bar window

Use the Console panel to inspect client-side errors.

Use the Network panel to inspect:

    Message payloads
    Connection timing

​
Common issues
​
Environment variables

MCP servers inherit only a subset of environment variables automatically, like USER, HOME, and PATH.

To override the default variables or provide your own, you can specify an env key in claude_desktop_config.json:

{
  "myserver": {
    "command": "mcp-server-myapp",
    "env": {
      "MYAPP_API_KEY": "some_key",
    }
  }
}

​
Server initialization

Common initialization problems:

    Path Issues
        Incorrect server executable path
        Missing required files
        Permission problems

    Configuration Errors
        Invalid JSON syntax
        Missing required fields
        Type mismatches

    Environment Problems
        Missing environment variables
        Incorrect variable values
        Permission restrictions

​
Connection problems

When servers fail to connect:

    Check Claude Desktop logs
    Verify server process is running
    Test standalone with Inspector
    Verify protocol compatibility

​
Implementing logging
​
Server-side logging

When building a server that uses the local stdio transport, all messages logged to stderr (standard error) will be captured by the host application (e.g., Claude Desktop) automatically.

Local MCP servers should not log messages to stdout (standard out), as this will interfere with protocol operation.

For all transports, you can also provide logging to the client by sending a log message notification:

typescript:

server.sendLoggingMessage({
  level: "info",
  data: "Server started successfully",
});


python:

server.request_context.session.send_log_message(
  level="info",
  data="Server started successfully",
)

Important events to log:

    Initialization steps
    Resource access
    Tool execution
    Error conditions
    Performance metrics

​
Client-side logging

In client applications:

    Enable debug logging
    Monitor network traffic
    Track message exchanges
    Record error states

​
Debugging workflow
​
Development cycle

    Initial Development
        Use Inspector for basic testing
        Implement core functionality
        Add logging points

    Integration Testing
        Test in Claude Desktop
        Monitor logs
        Check error handling

​
Testing changes

To test changes efficiently:

    Configuration changes: Restart Claude Desktop
    Server code changes: Use Command-R to reload
    Quick iteration: Use Inspector during development

​
Best practices
​
Logging strategy

    Structured Logging
        Use consistent formats
        Include context
        Add timestamps
        Track request IDs

    Error Handling
        Log stack traces
        Include error context
        Track error patterns
        Monitor recovery

    Performance Tracking
        Log operation timing
        Monitor resource usage
        Track message sizes
        Measure latency

​
Security considerations

When debugging:

    Sensitive Data
        Sanitize logs
        Protect credentials
        Mask personal information

    Access Control
        Verify permissions
        Check authentication
        Monitor access patterns

​
Getting help

When encountering issues:

    First Steps
        Check server logs
        Test with Inspector
        Review configuration
        Verify environment

    Support Channels
        GitHub issues
        GitHub discussions

    Providing Information
        Log excerpts
        Configuration files
        Steps to reproduce
        Environment details

​
Next steps
MCP Inspector

Learn to use the MCP Inspector
Tutorials
Inspector

In-depth guide to using the MCP Inspector for testing and debugging Model Context Protocol servers

The MCP Inspector is an interactive developer tool for testing and debugging MCP servers. While the Debugging Guide covers the Inspector as part of the overall debugging toolkit, this document provides a detailed exploration of the Inspector’s features and capabilities.
​
Getting started
​
Installation and basic usage

The Inspector runs directly through npx without requiring installation:

npx @modelcontextprotocol/inspector <command>

npx @modelcontextprotocol/inspector <command> <arg1> <arg2>

​
Inspecting servers from NPM or PyPi

A common way to start server packages from NPM or PyPi.

npx -y @modelcontextprotocol/inspector npx <package-name> <args>
# For example
npx -y @modelcontextprotocol/inspector npx server-postgres postgres://127.0.0.1/testdb

​
Inspecting locally developed servers

To inspect servers locally developed or downloaded as a repository, the most common way is:

npx @modelcontextprotocol/inspector node path/to/server/index.js args...

Please carefully read any attached README for the most accurate instructions.
​
Feature overview

The MCP Inspector interface

The Inspector provides several features for interacting with your MCP server:
​
Server connection pane

    Allows selecting the transport for connecting to the server
    For local servers, supports customizing the command-line arguments and environment

​
Resources tab

    Lists all available resources
    Shows resource metadata (MIME types, descriptions)
    Allows resource content inspection
    Supports subscription testing

​
Prompts tab

    Displays available prompt templates
    Shows prompt arguments and descriptions
    Enables prompt testing with custom arguments
    Previews generated messages

​
Tools tab

    Lists available tools
    Shows tool schemas and descriptions
    Enables tool testing with custom inputs
    Displays tool execution results

​
Notifications pane

    Presents all logs recorded from the server
    Shows notifications received from the server

​
Best practices
​
Development workflow

    Start Development
        Launch Inspector with your server
        Verify basic connectivity
        Check capability negotiation

    Iterative testing
        Make server changes
        Rebuild the server
        Reconnect the Inspector
        Test affected features
        Monitor messages

    Test edge cases
        Invalid inputs
        Missing prompt arguments
        Concurrent operations
        Verify error handling and error responses

​
Next steps
Inspector Repository

Check out the MCP Inspector source code
Debugging Guide

Learn about broader debugging strategies

Was this page helpful?
Concepts
Core architecture

Understand how MCP connects clients, servers, and LLMs

The Model Context Protocol (MCP) is built on a flexible, extensible architecture that enables seamless communication between LLM applications and integrations. This document covers the core architectural components and concepts.
​
Overview

MCP follows a client-server architecture where:

    Hosts are LLM applications (like Claude Desktop or IDEs) that initiate connections
    Clients maintain 1:1 connections with servers, inside the host application
    Servers provide context, tools, and prompts to clients

Server Process
Server Process
 Host (e.g., Claude Desktop) 
Transport Layer
Transport Layer
MCP Server
MCP Server
MCP Client
MCP Client
​
Core components
​
Protocol layer

The protocol layer handles message framing, request/response linking, and high-level communication patterns.

class Protocol<Request, Notification, Result> {
    // Handle incoming requests
    setRequestHandler<T>(schema: T, handler: (request: T, extra: RequestHandlerExtra) => Promise<Result>): void

    // Handle incoming notifications
    setNotificationHandler<T>(schema: T, handler: (notification: T) => Promise<void>): void

    // Send requests and await responses
    request<T>(request: Request, schema: T, options?: RequestOptions): Promise<T>

    // Send one-way notifications
    notification(notification: Notification): Promise<void>
}

Key classes include:

    Protocol
    Client
    Server

​
Transport layer

The transport layer handles the actual communication between clients and servers. MCP supports multiple transport mechanisms:

    Stdio transport
        Uses standard input/output for communication
        Ideal for local processes

    HTTP with SSE transport
        Uses Server-Sent Events for server-to-client messages
        HTTP POST for client-to-server messages

All transports use JSON-RPC 2.0 to exchange messages. See the specification for detailed information about the Model Context Protocol message format.
​
Message types

MCP has these main types of messages:

    Requests expect a response from the other side:

interface Request {
  method: string;
  params?: { ... };
}

Notifications are one-way messages that don’t expect a response:

interface Notification {
  method: string;
  params?: { ... };
}

Results are successful responses to requests:

interface Result {
  [key: string]: unknown;
}

Errors indicate that a request failed:

    interface Error {
      code: number;
      message: string;
      data?: unknown;
    }

​
Connection lifecycle
​
1. Initialization
ServerClientServerClientConnection ready for useinitialize requestinitialize responseinitialized notification

    Client sends initialize request with protocol version and capabilities
    Server responds with its protocol version and capabilities
    Client sends initialized notification as acknowledgment
    Normal message exchange begins

​
2. Message exchange

After initialization, the following patterns are supported:

    Request-Response: Client or server sends requests, the other responds
    Notifications: Either party sends one-way messages

​
3. Termination

Either party can terminate the connection:

    Clean shutdown via close()
    Transport disconnection
    Error conditions

​
Error handling

MCP defines these standard error codes:

enum ErrorCode {
  // Standard JSON-RPC error codes
  ParseError = -32700,
  InvalidRequest = -32600,
  MethodNotFound = -32601,
  InvalidParams = -32602,
  InternalError = -32603
}

SDKs and applications can define their own error codes above -32000.

Errors are propagated through:

    Error responses to requests
    Error events on transports
    Protocol-level error handlers

​
Implementation example

Here’s a basic example of implementing an MCP server:
typescript:

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";

const server = new Server({
  name: "example-server",
  version: "1.0.0"
}, {
  capabilities: {
    resources: {}
  }
});

// Handle requests
server.setRequestHandler(ListResourcesRequestSchema, async () => {
  return {
    resources: [
      {
        uri: "example://resource",
        name: "Example Resource"
      }
    ]
  };
});

// Connect transport
const transport = new StdioServerTransport();
await server.connect(transport);

​python:

import asyncio
import mcp.types as types
from mcp.server import Server
from mcp.server.stdio import stdio_server

app = Server("example-server")

@app.list_resources()
async def list_resources() -> list[types.Resource]:
    return [
        types.Resource(
            uri="example://resource",
            name="Example Resource"
        )
    ]

async def main():
    async with stdio_server() as streams:
        await app.run(
            streams[0],
            streams[1],
            app.create_initialization_options()
        )

if __name__ == "__main__":
    asyncio.run(main)
	
	
Best practices
​
Transport selection

    Local communication
        Use stdio transport for local processes
        Efficient for same-machine communication
        Simple process management

    Remote communication
        Use SSE for scenarios requiring HTTP compatibility
        Consider security implications including authentication and authorization

​
Message handling

    Request processing
        Validate inputs thoroughly
        Use type-safe schemas
        Handle errors gracefully
        Implement timeouts

    Progress reporting
        Use progress tokens for long operations
        Report progress incrementally
        Include total progress when known

    Error management
        Use appropriate error codes
        Include helpful error messages
        Clean up resources on errors

​
Security considerations

    Transport security
        Use TLS for remote connections
        Validate connection origins
        Implement authentication when needed

    Message validation
        Validate all incoming messages
        Sanitize inputs
        Check message size limits
        Verify JSON-RPC format

    Resource protection
        Implement access controls
        Validate resource paths
        Monitor resource usage
        Rate limit requests

    Error handling
        Don’t leak sensitive information
        Log security-relevant errors
        Implement proper cleanup
        Handle DoS scenarios

​
Debugging and monitoring

    Logging
        Log protocol events
        Track message flow
        Monitor performance
        Record errors

    Diagnostics
        Implement health checks
        Monitor connection state
        Track resource usage
        Profile performance

    Testing
        Test different transports
        Verify error handling
        Check edge cases
        Load test servers


Concepts
Resources

Expose data and content from your servers to LLMs

Resources are a core primitive in the Model Context Protocol (MCP) that allow servers to expose data and content that can be read by clients and used as context for LLM interactions.

Resources are designed to be application-controlled, meaning that the client application can decide how and when they should be used. Different MCP clients may handle resources differently. For example:

    Claude Desktop currently requires users to explicitly select resources before they can be used
    Other clients might automatically select resources based on heuristics
    Some implementations may even allow the AI model itself to determine which resources to use

Server authors should be prepared to handle any of these interaction patterns when implementing resource support. In order to expose data to models automatically, server authors should use a model-controlled primitive such as Tools.
​
Overview

Resources represent any kind of data that an MCP server wants to make available to clients. This can include:

    File contents
    Database records
    API responses
    Live system data
    Screenshots and images
    Log files
    And more

Each resource is identified by a unique URI and can contain either text or binary data.
​
Resource URIs

Resources are identified using URIs that follow this format:

[protocol]://[host]/[path]

For example:

    file:///home/user/documents/report.pdf
    postgres://database/customers/schema
    screen://localhost/display1

The protocol and path structure is defined by the MCP server implementation. Servers can define their own custom URI schemes.
​
Resource types

Resources can contain two types of content:
​
Text resources

Text resources contain UTF-8 encoded text data. These are suitable for:

    Source code
    Configuration files
    Log files
    JSON/XML data
    Plain text

​
Binary resources

Binary resources contain raw binary data encoded in base64. These are suitable for:

    Images
    PDFs
    Audio files
    Video files
    Other non-text formats

​
Resource discovery

Clients can discover available resources through two main methods:
​
Direct resources

Servers expose a list of concrete resources via the resources/list endpoint. Each resource includes:

{
  uri: string;           // Unique identifier for the resource
  name: string;          // Human-readable name
  description?: string;  // Optional description
  mimeType?: string;     // Optional MIME type
}

​
Resource templates

For dynamic resources, servers can expose URI templates that clients can use to construct valid resource URIs:

{
  uriTemplate: string;   // URI template following RFC 6570
  name: string;          // Human-readable name for this type
  description?: string;  // Optional description
  mimeType?: string;     // Optional MIME type for all matching resources
}

​
Reading resources

To read a resource, clients make a resources/read request with the resource URI.

The server responds with a list of resource contents:

{
  contents: [
    {
      uri: string;        // The URI of the resource
      mimeType?: string;  // Optional MIME type

      // One of:
      text?: string;      // For text resources
      blob?: string;      // For binary resources (base64 encoded)
    }
  ]
}

Servers may return multiple resources in response to one resources/read request. This could be used, for example, to return a list of files inside a directory when the directory is read.
​
Resource updates

MCP supports real-time updates for resources through two mechanisms:
​
List changes

Servers can notify clients when their list of available resources changes via the notifications/resources/list_changed notification.
​
Content changes

Clients can subscribe to updates for specific resources:

    Client sends resources/subscribe with resource URI
    Server sends notifications/resources/updated when the resource changes
    Client can fetch latest content with resources/read
    Client can unsubscribe with resources/unsubscribe

​
Example implementation

Here’s a simple example of implementing resource support in an MCP server:
typescript:

const server = new Server({
  name: "example-server",
  version: "1.0.0"
}, {
  capabilities: {
    resources: {}
  }
});

// List available resources
server.setRequestHandler(ListResourcesRequestSchema, async () => {
  return {
    resources: [
      {
        uri: "file:///logs/app.log",
        name: "Application Logs",
        mimeType: "text/plain"
      }
    ]
  };
});

// Read resource contents
server.setRequestHandler(ReadResourceRequestSchema, async (request) => {
  const uri = request.params.uri;

  if (uri === "file:///logs/app.log") {
    const logContents = await readLogFile();
    return {
      contents: [
        {
          uri,
          mimeType: "text/plain",
          text: logContents
        }
      ]
    };
  }

  throw new Error("Resource not found");
});

python:

app = Server("example-server")

@app.list_resources()
async def list_resources() -> list[types.Resource]:
    return [
        types.Resource(
            uri="file:///logs/app.log",
            name="Application Logs",
            mimeType="text/plain"
        )
    ]

@app.read_resource()
async def read_resource(uri: AnyUrl) -> str:
    if str(uri) == "file:///logs/app.log":
        log_contents = await read_log_file()
        return log_contents

    raise ValueError("Resource not found")

# Start server
async with stdio_server() as streams:
    await app.run(
        streams[0],
        streams[1],
        app.create_initialization_options()
    )
	
	
​
Best practices

When implementing resource support:

    Use clear, descriptive resource names and URIs
    Include helpful descriptions to guide LLM understanding
    Set appropriate MIME types when known
    Implement resource templates for dynamic content
    Use subscriptions for frequently changing resources
    Handle errors gracefully with clear error messages
    Consider pagination for large resource lists
    Cache resource contents when appropriate
    Validate URIs before processing
    Document your custom URI schemes

​
Security considerations

When exposing resources:

    Validate all resource URIs
    Implement appropriate access controls
    Sanitize file paths to prevent directory traversal
    Be cautious with binary data handling
    Consider rate limiting for resource reads
    Audit resource access
    Encrypt sensitive data in transit
    Validate MIME types
    Implement timeouts for long-running reads
    Handle resource cleanup appropriately


Concepts
Prompts

Create reusable prompt templates and workflows

Prompts enable servers to define reusable prompt templates and workflows that clients can easily surface to users and LLMs. They provide a powerful way to standardize and share common LLM interactions.

Prompts are designed to be user-controlled, meaning they are exposed from servers to clients with the intention of the user being able to explicitly select them for use.
​
Overview

Prompts in MCP are predefined templates that can:

    Accept dynamic arguments
    Include context from resources
    Chain multiple interactions
    Guide specific workflows
    Surface as UI elements (like slash commands)

​
Prompt structure

Each prompt is defined with:

{
  name: string;              // Unique identifier for the prompt
  description?: string;      // Human-readable description
  arguments?: [              // Optional list of arguments
    {
      name: string;          // Argument identifier
      description?: string;  // Argument description
      required?: boolean;    // Whether argument is required
    }
  ]
}

​
Discovering prompts

Clients can discover available prompts through the prompts/list endpoint:

// Request
{
  method: "prompts/list"
}

// Response
{
  prompts: [
    {
      name: "analyze-code",
      description: "Analyze code for potential improvements",
      arguments: [
        {
          name: "language",
          description: "Programming language",
          required: true
        }
      ]
    }
  ]
}

​
Using prompts

To use a prompt, clients make a prompts/get request:

// Request
{
  method: "prompts/get",
  params: {
    name: "analyze-code",
    arguments: {
      language: "python"
    }
  }
}

// Response
{
  description: "Analyze Python code for potential improvements",
  messages: [
    {
      role: "user",
      content: {
        type: "text",
        text: "Please analyze the following Python code for potential improvements:\n\n```python\ndef calculate_sum(numbers):\n    total = 0\n    for num in numbers:\n        total = total + num\n    return total\n\nresult = calculate_sum([1, 2, 3, 4, 5])\nprint(result)\n```"
      }
    }
  ]
}

​
Dynamic prompts

Prompts can be dynamic and include:
​
Embedded resource context

{
  "name": "analyze-project",
  "description": "Analyze project logs and code",
  "arguments": [
    {
      "name": "timeframe",
      "description": "Time period to analyze logs",
      "required": true
    },
    {
      "name": "fileUri",
      "description": "URI of code file to review",
      "required": true
    }
  ]
}

When handling the prompts/get request:

{
  "messages": [
    {
      "role": "user",
      "content": {
        "type": "text",
        "text": "Analyze these system logs and the code file for any issues:"
      }
    },
    {
      "role": "user",
      "content": {
        "type": "resource",
        "resource": {
          "uri": "logs://recent?timeframe=1h",
          "text": "[2024-03-14 15:32:11] ERROR: Connection timeout in network.py:127\n[2024-03-14 15:32:15] WARN: Retrying connection (attempt 2/3)\n[2024-03-14 15:32:20] ERROR: Max retries exceeded",
          "mimeType": "text/plain"
        }
      }
    },
    {
      "role": "user",
      "content": {
        "type": "resource",
        "resource": {
          "uri": "file:///path/to/code.py",
          "text": "def connect_to_service(timeout=30):\n    retries = 3\n    for attempt in range(retries):\n        try:\n            return establish_connection(timeout)\n        except TimeoutError:\n            if attempt == retries - 1:\n                raise\n            time.sleep(5)\n\ndef establish_connection(timeout):\n    # Connection implementation\n    pass",
          "mimeType": "text/x-python"
        }
      }
    }
  ]
}

​
Multi-step workflows

const debugWorkflow = {
  name: "debug-error",
  async getMessages(error: string) {
    return [
      {
        role: "user",
        content: {
          type: "text",
          text: `Here's an error I'm seeing: ${error}`
        }
      },
      {
        role: "assistant",
        content: {
          type: "text",
          text: "I'll help analyze this error. What have you tried so far?"
        }
      },
      {
        role: "user",
        content: {
          type: "text",
          text: "I've tried restarting the service, but the error persists."
        }
      }
    ];
  }
};

​
Example implementation

Here’s a complete example of implementing prompts in an MCP server:
typescript:

import { Server } from "@modelcontextprotocol/sdk/server";
import {
  ListPromptsRequestSchema,
  GetPromptRequestSchema
} from "@modelcontextprotocol/sdk/types";

const PROMPTS = {
  "git-commit": {
    name: "git-commit",
    description: "Generate a Git commit message",
    arguments: [
      {
        name: "changes",
        description: "Git diff or description of changes",
        required: true
      }
    ]
  },
  "explain-code": {
    name: "explain-code",
    description: "Explain how code works",
    arguments: [
      {
        name: "code",
        description: "Code to explain",
        required: true
      },
      {
        name: "language",
        description: "Programming language",
        required: false
      }
    ]
  }
};

const server = new Server({
  name: "example-prompts-server",
  version: "1.0.0"
}, {
  capabilities: {
    prompts: {}
  }
});

// List available prompts
server.setRequestHandler(ListPromptsRequestSchema, async () => {
  return {
    prompts: Object.values(PROMPTS)
  };
});

// Get specific prompt
server.setRequestHandler(GetPromptRequestSchema, async (request) => {
  const prompt = PROMPTS[request.params.name];
  if (!prompt) {
    throw new Error(`Prompt not found: ${request.params.name}`);
  }

  if (request.params.name === "git-commit") {
    return {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Generate a concise but descriptive commit message for these changes:\n\n${request.params.arguments?.changes}`
          }
        }
      ]
    };
  }

  if (request.params.name === "explain-code") {
    const language = request.params.arguments?.language || "Unknown";
    return {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Explain how this ${language} code works:\n\n${request.params.arguments?.code}`
          }
        }
      ]
    };
  }

  throw new Error("Prompt implementation not found");
});

​python:

from mcp.server import Server
import mcp.types as types

# Define available prompts
PROMPTS = {
    "git-commit": types.Prompt(
        name="git-commit",
        description="Generate a Git commit message",
        arguments=[
            types.PromptArgument(
                name="changes",
                description="Git diff or description of changes",
                required=True
            )
        ],
    ),
    "explain-code": types.Prompt(
        name="explain-code",
        description="Explain how code works",
        arguments=[
            types.PromptArgument(
                name="code",
                description="Code to explain",
                required=True
            ),
            types.PromptArgument(
                name="language",
                description="Programming language",
                required=False
            )
        ],
    )
}

# Initialize server
app = Server("example-prompts-server")

@app.list_prompts()
async def list_prompts() -> list[types.Prompt]:
    return list(PROMPTS.values())

@app.get_prompt()
async def get_prompt(
    name: str, arguments: dict[str, str] | None = None
) -> types.GetPromptResult:
    if name not in PROMPTS:
        raise ValueError(f"Prompt not found: {name}")

    if name == "git-commit":
        changes = arguments.get("changes") if arguments else ""
        return types.GetPromptResult(
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=f"Generate a concise but descriptive commit Here’s a simple example of implementing resource support in an MCP server "
                        f"for these changes:\n\n{changes}"
                    )
                )
            ]
        )

    if name == "explain-code":
        code = arguments.get("code") if arguments else ""
        language = arguments.get("language", "Unknown") if arguments else "Unknown"
        return types.GetPromptResult(
            messages=[
                types.PromptMessage(
                    role="user",
                    content=types.TextContent(
                        type="text",
                        text=f"Explain how this {language} code works:\n\n{code}"
                    )
                )
            ]
        )

    raise ValueError("Prompt implementation not found")
	
	
Best practices

When implementing prompts:

    Use clear, descriptive prompt names
    Provide detailed descriptions for prompts and arguments
    Validate all required arguments
    Handle missing arguments gracefully
    Consider versioning for prompt templates
    Cache dynamic content when appropriate
    Implement error handling
    Document expected argument formats
    Consider prompt composability
    Test prompts with various inputs

​
UI integration

Prompts can be surfaced in client UIs as:

    Slash commands
    Quick actions
    Context menu items
    Command palette entries
    Guided workflows
    Interactive forms
Updates and changes

Servers can notify clients about prompt changes:

    Server capability: prompts.listChanged
    Notification: notifications/prompts/list_changed
    Client re-fetches prompt list

​
Security considerations

When implementing prompts:

    Validate all arguments
    Sanitize user input
    Consider rate limiting
    Implement access controls
    Audit prompt usage
    Handle sensitive data appropriately
    Validate generated content
    Implement timeouts
    Consider prompt injection risks
    Document security requirements

Concepts
Tools

Enable LLMs to perform actions through your server

Tools are a powerful primitive in the Model Context Protocol (MCP) that enable servers to expose executable functionality to clients. Through tools, LLMs can interact with external systems, perform computations, and take actions in the real world.

Tools are designed to be model-controlled, meaning that tools are exposed from servers to clients with the intention of the AI model being able to automatically invoke them (with a human in the loop to grant approval).
​
Overview

Tools in MCP allow servers to expose executable functions that can be invoked by clients and used by LLMs to perform actions. Key aspects of tools include:

    Discovery: Clients can list available tools through the tools/list endpoint
    Invocation: Tools are called using the tools/call endpoint, where servers perform the requested operation and return results
    Flexibility: Tools can range from simple calculations to complex API interactions

Like resources, tools are identified by unique names and can include descriptions to guide their usage. However, unlike resources, tools represent dynamic operations that can modify state or interact with external systems.
​
Tool definition structure

Each tool is defined with the following structure:

{
  name: string;          // Unique identifier for the tool
  description?: string;  // Human-readable description
  inputSchema: {         // JSON Schema for the tool's parameters
    type: "object",
    properties: { ... }  // Tool-specific parameters
  }
}

​
Implementing tools

Here’s an example of implementing a basic tool in an MCP server:

typescript:

const server = new Server({
  name: "example-server",
  version: "1.0.0"
}, {
  capabilities: {
    tools: {}
  }
});

// Define available tools
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [{
      name: "calculate_sum",
      description: "Add two numbers together",
      inputSchema: {
        type: "object",
        properties: {
          a: { type: "number" },
          b: { type: "number" }
        },
        required: ["a", "b"]
      }
    }]
  };
});

// Handle tool execution
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  if (request.params.name === "calculate_sum") {
    const { a, b } = request.params.arguments;
    return {
      toolResult: a + b
    };
  }
  throw new Error("Tool not found");
});

​python:

app = Server("example-server")

@app.list_tools()
async def list_tools() -> list[types.Tool]:
    return [
        types.Tool(
            name="calculate_sum",
            description="Add two numbers together",
            inputSchema={
                "type": "object",
                "properties": {
                    "a": {"type": "number"},
                    "b": {"type": "number"}
                },
                "required": ["a", "b"]
            }
        )
    ]

@app.call_tool()
async def call_tool(
    name: str,
    arguments: dict
) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:
    if name == "calculate_sum":
        a = arguments["a"]
        b = arguments["b"]
        result = a + b
        return [types.TextContent(type="text", text=str(result))]
    raise ValueError(f"Tool not found: {name}")
	
	
	
Example tool patterns

Here are some examples of types of tools that a server could provide:
​
System operations

Tools that interact with the local system:

{
  name: "execute_command",
  description: "Run a shell command",
  inputSchema: {
    type: "object",
    properties: {
      command: { type: "string" },
      args: { type: "array", items: { type: "string" } }
    }
  }
}

​
API integrations

Tools that wrap external APIs:

{
  name: "github_create_issue",
  description: "Create a GitHub issue",
  inputSchema: {
    type: "object",
    properties: {
      title: { type: "string" },
      body: { type: "string" },
      labels: { type: "array", items: { type: "string" } }
    }
  }
}

​
Data processing

Tools that transform or analyze data:

{
  name: "analyze_csv",
  description: "Analyze a CSV file",
  inputSchema: {
    type: "object",
    properties: {
      filepath: { type: "string" },
      operations: {
        type: "array",
        items: {
          enum: ["sum", "average", "count"]
        }
      }
    }
  }
}

​
Best practices

When implementing tools:

    Provide clear, descriptive names and descriptions
    Use detailed JSON Schema definitions for parameters
    Include examples in tool descriptions to demonstrate how the model should use them
    Implement proper error handling and validation
    Use progress reporting for long operations
    Keep tool operations focused and atomic
    Document expected return value structures
    Implement proper timeouts
    Consider rate limiting for resource-intensive operations
    Log tool usage for debugging and monitoring

​
Security considerations

When exposing tools:
​
Input validation

    Validate all parameters against the schema
    Sanitize file paths and system commands
    Validate URLs and external identifiers
    Check parameter sizes and ranges
    Prevent command injection

​
Access control

    Implement authentication where needed
    Use appropriate authorization checks
    Audit tool usage
    Rate limit requests
    Monitor for abuse

​
Error handling

    Don’t expose internal errors to clients
    Log security-relevant errors
    Handle timeouts appropriately
    Clean up resources after errors
    Validate return values

​
Tool discovery and updates

MCP supports dynamic tool discovery:

    Clients can list available tools at any time
    Servers can notify clients when tools change using notifications/tools/list_changed
    Tools can be added or removed during runtime
    Tool definitions can be updated (though this should be done carefully)

​
Error handling

Tool errors should be reported within the result object, not as MCP protocol-level errors. This allows the LLM to see and potentially handle the error. When a tool encounters an error:

    Set isError to true in the result
    Include error details in the content array

Here’s an example of proper error handling for tools:

typescript:

try {
  // Tool operation
  const result = performOperation();
  return {
    content: [
      {
        type: "text",
        text: `Operation successful: ${result}`
      }
    ]
  };
} catch (error) {
  return {
    isError: true,
    content: [
      {
        type: "text",
        text: `Error: ${error.message}`
      }
    ]
  };
}

python:
try:
    # Tool operation
    result = perform_operation()
    return types.CallToolResult(
        content=[
            types.TextContent(
                type="text",
                text=f"Operation successful: {result}"
            )
        ]
    )
except Exception as error:
    return types.CallToolResult(
        isError=True,
        content=[
            types.TextContent(
                type="text",
                text=f"Error: {str(error)}"
            )
        ]
    )
	
	
This approach allows the LLM to see that an error occurred and potentially take corrective action or request human intervention.
​
Testing tools

A comprehensive testing strategy for MCP tools should cover:

    Functional testing: Verify tools execute correctly with valid inputs and handle invalid inputs appropriately
    Integration testing: Test tool interaction with external systems using both real and mocked dependencies
    Security testing: Validate authentication, authorization, input sanitization, and rate limiting
    Performance testing: Check behavior under load, timeout handling, and resource cleanup
    Error handling: Ensure tools properly report errors through the MCP protocol and clean up resources

Troubleshooting

Getting logs from Claude for Desktop

Claude.app logging related to MCP is written to log files in ~/Library/Logs/Claude:

    mcp.log will contain general logging about MCP connections and connection failures.
    Files named mcp-server-SERVERNAME.log will contain error (stderr) logging from the named server.

You can run the following command to list recent logs and follow along with any new ones:

# Check Claude's logs for errors
tail -n 20 -f ~/Library/Logs/Claude/mcp*.log

Server not showing up in Claude

    Check your desktop_config.json file syntax
    Make sure the path to your project is absolute and not relative
    Restart Claude for Desktop completely

Tool calls failing silently

If Claude attempts to use the tools but they fail:

    Check Claude’s logs for errors
    Verify your server builds and runs without errors
    Try restarting Claude for Desktop

None of this is working. What do I do?

Please refer to our debugging guide for better debugging tools and more detailed guidance.

Error: Failed to retrieve grid point data

This usually means either:

    The coordinates are outside the US
    The NWS API is having issues
    You’re being rate limited

Fix:

    Verify you’re using US coordinates
    Add a small delay between requests
    Check the NWS API status page

Error: No active alerts for [STATE]

This isn’t an error - it just means there are no current weather alerts for that state. Try a different state or check during severe weather.


